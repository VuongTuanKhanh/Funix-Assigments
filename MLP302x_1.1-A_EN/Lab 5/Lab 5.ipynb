{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 5.ipynb","provenance":[],"collapsed_sections":["cnD1ecF0Mw0u","wRSBur8HMw02","JBmiSN8sMw1A","nyLeYGvlMw1J","9Zv3CFZBMw1s"],"mount_file_id":"1pLWNWZT8p7AjgoUeS687nmDgLOAoxNEr","authorship_tag":"ABX9TyM0ejScCCeWiwK+2eTvRXhL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cnD1ecF0Mw0u"},"source":["# Feature Selection and LASSO (Interpretation)"]},{"cell_type":"markdown","metadata":{"id":"N6oYlO_fMw0z"},"source":["In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (in sklearn, obviously). You will:\n","* Run LASSO with different L1 penalties.\n","* Choose best L1 penalty using a validation set.\n","* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n","\n","In the next exercise, you will implement your own LASSO solver, using coordinate descent. "]},{"cell_type":"markdown","metadata":{"id":"wRSBur8HMw02"},"source":["## The usual"]},{"cell_type":"code","metadata":{"id":"MjaAEidBMw05","executionInfo":{"status":"ok","timestamp":1614122536230,"user_tz":-420,"elapsed":2311,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["import sklearn\n","import pandas as pd\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBmiSN8sMw1A"},"source":["## Load in house sales data\n","\n","Dataset is from house sales in King County, the region where the city of Seattle, WA is located. *I am so surprised.*"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7cDLiqAIMw1D","executionInfo":{"status":"ok","timestamp":1614122538368,"user_tz":-420,"elapsed":4438,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["full_data = pd.read_csv(\"/content/drive/MyDrive/FUNIX Progress/MLP302x_1.1-A_EN/data/kc_house_data.csv\", index_col=0)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyLeYGvlMw1J"},"source":["## Create new features"]},{"cell_type":"markdown","metadata":{"id":"XSMOdbHVMw1L"},"source":["As in Lab 2 (*lab-2.ipynb*), we consider features that are some transformations of inputs."]},{"cell_type":"code","metadata":{"id":"mYzffDTvMw1M","executionInfo":{"status":"ok","timestamp":1614122538374,"user_tz":-420,"elapsed":4438,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["from math import log, sqrt\n","full_data['sqft_living_sqrt'] = full_data['sqft_living'].map(sqrt)\n","full_data['sqft_lot_sqrt'] = full_data['sqft_lot'].map(sqrt)\n","full_data['bedrooms_square'] = full_data['bedrooms'] ** 2\n","\n","# In the dataset, 'floors' was defined with type string, \n","# so we'll convert them to float, before creating a new feature.\n","full_data['floors'] = full_data['floors'].astype(float) \n","full_data['floors_square'] = full_data['floors'] ** 2"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z672nh0kMw1S"},"source":["* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n","* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."]},{"cell_type":"markdown","metadata":{"id":"VDo2w5chMw1U"},"source":["# Learn regression weights with L1 penalty"]},{"cell_type":"markdown","metadata":{"id":"u-xMl9bUMw1V"},"source":["Let us fit a model with all the features available, plus the features we just created above."]},{"cell_type":"code","metadata":{"id":"OYAUT1PLMw1W","executionInfo":{"status":"ok","timestamp":1614122538375,"user_tz":-420,"elapsed":4435,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["all_features = ['bedrooms', 'bedrooms_square',\n","            'bathrooms',\n","            'sqft_living', 'sqft_living_sqrt',\n","            'sqft_lot', 'sqft_lot_sqrt',\n","            'floors', 'floors_square',\n","            'waterfront', 'view', 'condition', 'grade',\n","            'sqft_above',\n","            'sqft_basement',\n","            'yr_built', 'yr_renovated']"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7-592ikMw1e"},"source":["Applying L1 penalty requires adding an extra parameter (`alpha=l1_penalty`) to the sklearn model `Lasso`. (Other tools may have separate implementations of LASSO). Much like L2/Ridge Regression, the features should be scaled to ensure equal attention inbetween."]},{"cell_type":"code","metadata":{"id":"KlTbDC0RMw1g","executionInfo":{"status":"ok","timestamp":1614122538377,"user_tz":-420,"elapsed":4433,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["from sklearn.linear_model import Lasso\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","l1_penalty=5e4\n","full_features = scaler.fit_transform(full_data[all_features].values)\n","full_labels = full_data['price'].values\n","model = Lasso(alpha=l1_penalty).fit(full_features, full_labels)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rH4847CcMw1m"},"source":["Find what features had non-zero weight."]},{"cell_type":"code","metadata":{"id":"HElqPj8LMw1n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614122539510,"user_tz":-420,"elapsed":5552,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"35d0a12b-a13f-4699-c3d1-40d76f848130"},"source":["# Do you know that even numpy has built-in boolean selector?\r\n","model.coef_"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([     0.        ,      0.        ,      0.        , 132571.09360631,\n","            0.        ,     -0.        ,     -0.        ,      0.        ,\n","            0.        ,  14623.33961421,  29004.06421249,      0.        ,\n","        90207.54789031,      0.        ,      0.        , -10722.34912003,\n","            0.        ])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Z0gF5MD_Mw1r"},"source":["Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n","\n","***QUIZ QUESTION***:\n","According to this list of weights, which of the features have been chosen? "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6E-sMNgycmhX","executionInfo":{"status":"ok","timestamp":1614122539515,"user_tz":-420,"elapsed":5544,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"eef1e6a4-6fb1-4b79-db38-56f36ce08f32"},"source":["[all_features[idx] for idx, val in enumerate(model.coef_) if val != 0]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sqft_living', 'waterfront', 'view', 'grade', 'yr_built']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"9Zv3CFZBMw1s"},"source":["# Selecting an L1 penalty"]},{"cell_type":"markdown","metadata":{"id":"Oc6Vm3RfMw1t"},"source":["To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n","* Split our sales data into 2 sets: training and test (9/1)\n","* Further split our training data into two sets: train, validation (5/5)\n","\n","Be *very* careful that you use seed = 1 to ensure you get the same answer!"]},{"cell_type":"code","metadata":{"id":"U41_wuIcMw1v","executionInfo":{"status":"ok","timestamp":1614122539516,"user_tz":-420,"elapsed":5541,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["from sklearn.model_selection import train_test_split\n","train_and_validation, test_data = train_test_split(full_data, train_size=0.9, test_size=0.1, random_state=1)\n","train_data, validate_data = train_test_split(full_data, train_size=0.5, test_size=0.5, random_state=1)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kCoG3VwMw10"},"source":["Next, we write a loop that does the following:\n","* For `l1_penalty` in 21 steps range between [1, 10^9] (use `np.logspace(0, 9, num=21)`.)\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n","    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n","* Report which `l1_penalty` produced the lowest RSS on validation data."]},{"cell_type":"code","metadata":{"id":"kYf8zDJtMw11"},"source":["rss_arr = []\n","for l1_penalty in np.logspace(0, 9, num=21):\n","  features = scaler.fit_transform(train_data[all_features].values)\n","  labels = train_data['price'].values\n","\n","  model = Lasso(alpha=l1_penalty).fit(features, labels)\n","\n","  prediction = model.predict(validate_data[all_features].values)\n","\n","  error =  prediction - validate_data['price'].values\n","\n","  rss_arr.append(error.dot(error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAneHq6LMw15"},"source":["*** QUIZ QUESTION. *** What was the best value for the `l1_penalty`?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-eXrp8ZgwQ8","executionInfo":{"status":"ok","timestamp":1614122541600,"user_tz":-420,"elapsed":7600,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"91784db0-4cdc-4959-fa3a-c6840c666648"},"source":["best_l1_penalty = np.logspace(0, 9, num=21)[rss_arr.index(min(rss_arr))]\r\n","print(best_l1_penalty)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["251188.6431509582\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PcnSvUHyMw19"},"source":["***QUIZ QUESTION***\n","Also, using this value of L1 penalty, how many nonzero weights do you have?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCfiSWURhUHp","executionInfo":{"status":"ok","timestamp":1614122541605,"user_tz":-420,"elapsed":7586,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"84ae26aa-545a-4c7c-ce49-0c6381b792a2"},"source":["model = Lasso(alpha=best_l1_penalty).fit(features, labels)\r\n","model.coef_"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"_zNo9ywLMw2F"},"source":["# Limit the number of nonzero weights\n","\n","What if we absolutely wanted to limit ourselves to, say, 5 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."]},{"cell_type":"markdown","metadata":{"id":"jLijk-3UMw2G"},"source":["In this section, you are going to implement a simple, two phase procedure to achive this goal:\n","1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n","2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."]},{"cell_type":"code","metadata":{"id":"kOwQHIzMMw2H","executionInfo":{"status":"ok","timestamp":1614123073806,"user_tz":-420,"elapsed":721,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["max_nonzeros = 5"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMv9UnluMw2N"},"source":["## Exploring the larger range of values to find a narrow range with the desired sparsity\n","\n","Let's define a wide range of possible `l1_penalty_values`:"]},{"cell_type":"code","metadata":{"id":"vEaNt8VHMw2O","executionInfo":{"status":"ok","timestamp":1614123071801,"user_tz":-420,"elapsed":734,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["l1_penalty_values = np.logspace(3, 5, num=21)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mxrj8371Mw2S"},"source":["Now, implement a loop that search through this space of possible `l1_penalty` values:\n","\n","* For `l1_penalty` in `np.logspace(3, 5, num=21)`:\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n","    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n","        * *Hint: `model.coef_` gives you the coefficients/parameters you learned (barring intercept) in the form of numpy array. You can then use array\\[condition\\] for the list of values passing the condition. Or just use the builtin `np.count_nonzero()`"]},{"cell_type":"code","metadata":{"id":"_kFzVsaiMw2U","executionInfo":{"status":"ok","timestamp":1614123078050,"user_tz":-420,"elapsed":1517,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["nonzeros = []\r\n","for l1_penalty in l1_penalty_values:\r\n","  features = scaler.fit_transform(train_data[all_features].values)\r\n","  labels = train_data['price'].values\r\n","\r\n","  model = Lasso(alpha=l1_penalty).fit(features, labels)\r\n","\r\n","  nonzeros.append(np.count_nonzero(model.coef_))"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucgWXpj9Mw2Z"},"source":["Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n","\n","More formally, find:\n","* The smallest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n","    * Store this value in the variable `l1_penalty_min` (we will use it later)\n","* The biggest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n","    * Store this value in the variable `l1_penalty_max` (we will use it later)\n","\n","\n","*Hint: there are many ways to do this, e.g.:*\n","* Programmatically within the loop above\n","* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."]},{"cell_type":"code","metadata":{"id":"UBpwiXEVMw2b","executionInfo":{"status":"ok","timestamp":1614123028765,"user_tz":-420,"elapsed":750,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["l1_penalty_min = l1_penalty_values[[index for index, val in enumerate(nonzeros) if val == max_nonzeros][0]]\n","l1_penalty_max = l1_penalty_values[[index for index, val in enumerate(nonzeros) if val == max_nonzeros][-1]]"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-S2jtklaMw2g"},"source":["***QUIZ QUESTION.*** What values did you find for `l1_penalty_min` and `l1_penalty_max`, respectively? "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hVx69LCl8S7","executionInfo":{"status":"ok","timestamp":1614123217522,"user_tz":-420,"elapsed":733,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"d742be3d-3b16-461d-f059-49adf6d992c3"},"source":["print(\"- L1_penalty_min:\", l1_penalty_min)\r\n","print(\"- L1_penalty_max:\", l1_penalty_max)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["- L1_penalty_min: 19952.62314968879\n","- L1_penalty_max: 50118.72336272725\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-T_xUbJ5Mw2i"},"source":["## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n","\n","We will now explore the narrow region of `l1_penalty` values we found:"]},{"cell_type":"code","metadata":{"id":"mEICQwrAMw2k","executionInfo":{"status":"ok","timestamp":1614123234579,"user_tz":-420,"elapsed":718,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["l1_penalty_values = np.linspace(l1_penalty_min,l1_penalty_max,20)"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVkWedu9Mw2p"},"source":["* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty`.\n","    * Measure the RSS of the learned model on the VALIDATION set\n","\n","Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`."]},{"cell_type":"code","metadata":{"id":"gKsCKTP3Mw2q","executionInfo":{"status":"ok","timestamp":1614123332693,"user_tz":-420,"elapsed":1056,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}}},"source":["# See the quiz below as well\r\n","narrow_rss_arr = []\r\n","narrow_nonzeros = []\r\n","for l1_penalty in l1_penalty_values:\r\n","  features = scaler.fit_transform(train_data[all_features].values)\r\n","  labels = train_data['price'].values\r\n","\r\n","  model = Lasso(alpha=l1_penalty).fit(features, labels)\r\n","\r\n","  prediction = model.predict(validate_data[all_features].values)\r\n","\r\n","  error =  prediction - validate_data['price'].values\r\n","\r\n","  narrow_rss_arr.append(error.dot(error))\r\n","  narrow_nonzeros.append(np.count_nonzero(model.coef_))"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4vAuKXfMw2u"},"source":["***QUIZ QUESTIONS***\n","1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n","2. What features in this model have non-zero coefficients?"]},{"cell_type":"code","metadata":{"id":"VISUoAMIMw2v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614123536105,"user_tz":-420,"elapsed":728,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"facc0720-c5a6-4d1c-815c-c3bd4e80cb23"},"source":["idx = narrow_rss_arr.index(min(narrow_rss_arr))\r\n","print(\"- The value of l1_penalty with lowest RSS on the VALIDATION set:\", l1_penalty_values[idx])"],"execution_count":58,"outputs":[{"output_type":"stream","text":["- The value of l1_penalty with lowest RSS on the VALIDATION set: 19952.62314968879\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWXU3ylpQs5D","executionInfo":{"status":"ok","timestamp":1614123619032,"user_tz":-420,"elapsed":735,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"db8ad3b8-b3a5-43be-806e-f19b8a007c2c"},"source":["features = scaler.fit_transform(train_data[all_features].values)\r\n","labels = train_data['price'].values\r\n","\r\n","model = Lasso(alpha=l1_penalty_values[idx]).fit(features, labels)\r\n","[all_features[idx] for idx, val in enumerate(model.coef_) if val != 0]"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sqft_living', 'waterfront', 'view', 'grade', 'yr_built']"]},"metadata":{"tags":[]},"execution_count":59}]}]}