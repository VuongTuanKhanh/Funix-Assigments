{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exercise 1.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QjxqeVeIQVFFzM6BlTPmsm8-q2v0NSVz","authorship_tag":"ABX9TyMIUNPrM+o+z09KcduG/nZ+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"95s6_p4E5qC0"},"source":["# Predicting sentiment from product reviews\n","\n","\n","The goal of this first notebook is to explore logistic regression and feature engineering with existing GraphLab functions.\n","\n","In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n","\n","* Use pandas to do some feature engineering\n","* Train a logistic regression model to predict the sentiment of product reviews.\n","* Inspect the weights (coefficients) of a trained logistic regression model.\n","* Make a prediction (both class and probability) of sentiment for a new product review.\n","* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n","* Inspect the coefficients of the logistic regression model and interpret their meanings.\n","* Compare multiple logistic regression models.\n","\n","Let's get started!"]},{"cell_type":"code","metadata":{"id":"fIGwAkXt5qC2","executionInfo":{"status":"ok","timestamp":1619069959255,"user_tz":-420,"elapsed":1478,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["# Import some libs\n","\n","import pandas\n","import numpy as np\n","from sklearn.model_selection import train_test_split"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekzHZZtA5qC8","executionInfo":{"status":"ok","timestamp":1619069964598,"user_tz":-420,"elapsed":6812,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["# Import amazon_baby.csv data to pandas dataframe\n","products_df = pandas.read_csv('/content/drive/MyDrive/FUNIX Progress/MLP303x_1.1-A_EN/data/amazon_baby.csv')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kKtcXdxNkVW","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1619069964601,"user_tz":-420,"elapsed":6722,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"afa93594-31a6-46c9-ebc8-122a900b3643"},"source":["products_df.head(5)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>review</th>\n","      <th>rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Planetwise Flannel Wipes</td>\n","      <td>These flannel wipes are OK, but in my opinion ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Planetwise Wipe Pouch</td>\n","      <td>it came early and was not disappointed. i love...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Annas Dream Full Quilt with 2 Shams</td>\n","      <td>Very soft and comfortable and warmer than it l...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n","      <td>This is a product well worth the purchase.  I ...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n","      <td>All of my kids have cried non-stop when I trie...</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                name  ... rating\n","0                           Planetwise Flannel Wipes  ...      3\n","1                              Planetwise Wipe Pouch  ...      5\n","2                Annas Dream Full Quilt with 2 Shams  ...      5\n","3  Stop Pacifier Sucking without tears with Thumb...  ...      5\n","4  Stop Pacifier Sucking without tears with Thumb...  ...      5\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"2y1uB3Qj5qC_"},"source":["### 1. Preprocessing the data\n","Before using the review data, we need some preprocessing\n","<br>\n","First removew punctuation from words\n","<br>\n","For example: **good!** --> **good**, **resturant.** --> **resturant**"]},{"cell_type":"code","metadata":{"id":"FIQyoj4N5qDA","executionInfo":{"status":"ok","timestamp":1619069965066,"user_tz":-420,"elapsed":7181,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["def remove_punctuation(text):\n","    import string\n","    return text.translate(string.punctuation)\n","\n","products_df = products_df.fillna({'review':''})  # fill in N/A's in the review column\n","products_df['review_clean'] = products_df['review'].apply(remove_punctuation)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"agUaHvxO5qDG"},"source":["We will ignore all reviews with rating = 3, since they tend to have a neutral sentiment."]},{"cell_type":"code","metadata":{"id":"6ADFZDZ_5qDG","executionInfo":{"status":"ok","timestamp":1619069965068,"user_tz":-420,"elapsed":7179,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["products_df = products_df[products_df['rating'] != 3]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JvfKQYaD5qDK"},"source":["Now, we will assign reviews with a rating of 4 or higher to be positive reviews, while the ones with rating of 2 or lower are negative. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label. A good way is to create an anonymous function that converts a rating into a class label and then apply that function to every element in the rating column. In Pandas, you would use apply():"]},{"cell_type":"code","metadata":{"id":"TGn9OznU5qDL","executionInfo":{"status":"ok","timestamp":1619069965505,"user_tz":-420,"elapsed":7611,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["products_df['sentiment'] = products_df['rating'].apply(lambda rating : +1 if rating > 3 else -1)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Sc4Q4db5qDO"},"source":["Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set.\n","<br>\n","We set the seed to make sure we get the same results."]},{"cell_type":"code","metadata":{"id":"J5p2UcYF5qDP","executionInfo":{"status":"ok","timestamp":1619069965507,"user_tz":-420,"elapsed":7607,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["np.random.seed(1)\n","\n","train_data, test_data = train_test_split(products_df, test_size=0.2)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQpImXXJ5qDU"},"source":["### 2. Feature representation\n"," We will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as bag-of-word features. Since most words occur in only a few reviews, word count vectors are sparse. For this reason, scikit-learn and many other tools use sparse matrices to store a collection of word count vectors.\n","<br>\n","You can checkout the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit documentation."]},{"cell_type":"code","metadata":{"id":"iqav8_7c5qDU","executionInfo":{"status":"ok","timestamp":1619069976417,"user_tz":-420,"elapsed":18512,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n","# Use this token pattern to keep single-letter words\n","# First, learn vocabulary from the training data and assign columns to words\n","# Then convert the training data into a sparse matrix\n","train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n","# Second, convert the test data into a sparse matrix, using the same word-column mapping\n","test_matrix = vectorizer.transform(test_data['review_clean'])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_i3lTxy5qDY"},"source":["### 3. Train logistic regression model\n","Learn a logistic regression classifier using the training data. We are using scikit-learn, you should create an instance of the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class and then call the method fit() to train the classifier. This model should use the sparse word count matrix (**train_matrix**) as features and the column **sentiment** of **train_data** as the target. Use the default values for other parameters."]},{"cell_type":"code","metadata":{"id":"RpFHSVG-5qDZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986144,"user_tz":-420,"elapsed":28222,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"3bc4f25b-76a6-4518-863f-05309a9e82fd"},"source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression().fit(train_matrix, train_data['sentiment'])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FD-LUqAo5qDc"},"source":["Check train accuracy.\n","<br>\n","You should get the train accuracy: 0.9474291796913067"]},{"cell_type":"code","metadata":{"id":"noUDr6tD5qDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986145,"user_tz":-420,"elapsed":28206,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"9c6abf36-2dab-4366-a337-ca0863c5d1d6"},"source":["print('Train accuracy: {}'.format(clf.score(train_matrix, train_data['sentiment'])))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Train accuracy: 0.9474816530610715\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UUOsSYvu5qDf"},"source":["Check test accuracy. \n","You should get the test accuracy: 0.9345746754220263"]},{"cell_type":"code","metadata":{"id":"3f1iqZO45qDg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986147,"user_tz":-420,"elapsed":28191,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"2f2febe3-0d81-4a92-9dda-d9f579544c3c"},"source":["print ('Test accuracy: {}'.format(clf.score(test_matrix, test_data['sentiment'])))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Test accuracy: 0.9344247548799136\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TYgm4Tpb5qDl"},"source":["**Quiz question**: How many weights are >= 0?\n","<br>\n","**Your answer**:\n","<br>\n","`sum(1 for c in clf.coef_[0] if c >= 0)`"]},{"cell_type":"code","metadata":{"id":"ZpwKHYTVO1Jv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986148,"user_tz":-420,"elapsed":28175,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"15dd7a9e-d970-4ed0-812e-d98e309c94a2"},"source":["sum(1 for c in clf.coef_[0] if c >= 0)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40188"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"h7AsLLk85qDm"},"source":["### 4. Making prediction\n","\n","First, will calculate the scores for 6 samples from the **test_data**.\n","<br>\n","Recall from the lecture that: $$\\mbox{score}_i = \\mathbf{w}^\\intercal h(\\mathbf{x}_i)$$"]},{"cell_type":"code","metadata":{"id":"D3C3Nx4A5qDm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986150,"user_tz":-420,"elapsed":28158,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"ce615bbe-8570-4bb7-86e6-74fd0f14ea70"},"source":["sample_test_data = test_data[105:110]\n","print(sample_test_data['review_clean'])\n","print(sample_test_data['rating'])\n","print(sample_test_data['sentiment'])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["76151     It came to me with marks on it.I recently went...\n","164181    How did we survive before discovering Kidzikoo...\n","108065    My baby loves this jumper! He'll stay playing ...\n","176539    Looked at this set at Walmart for $50 and coul...\n","56851     Product was not working when received.  Batter...\n","Name: review_clean, dtype: object\n","76151     2\n","164181    5\n","108065    5\n","176539    5\n","56851     1\n","Name: rating, dtype: int64\n","76151    -1\n","164181    1\n","108065    1\n","176539    1\n","56851    -1\n","Name: sentiment, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s9ssg6rd5qDr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986151,"user_tz":-420,"elapsed":28140,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"4991b328-cb6c-401e-eebe-1467c72a2b0c"},"source":["sample_test_matrix = vectorizer.transform(sample_test_data['review_clean'])\n","scores = clf.decision_function(sample_test_matrix)\n","print(scores)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[-6.32190725  6.11927398  4.37471512  2.00441806 -5.66336975]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q1sb1wGO5qDw"},"source":["These scores can be used to make class predictions as follows:\n","<br>\n","$$\\hat{y}_i = \\begin{cases} +1 & \\text{if } \\mathbf{w}^\\intercal h(\\mathbf{x}_i) > 0\\\\ -1 &\\text{if } \\mathbf{w}^\\intercal h(\\mathbf{x}_i) \\leq 0 \\end{cases}$$"]},{"cell_type":"markdown","metadata":{"id":"Kw7x2dgV5qDx"},"source":["We can use the predict() function from scikit-learn."]},{"cell_type":"code","metadata":{"id":"MOtqjfvW5qDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986152,"user_tz":-420,"elapsed":28124,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"40c37f86-2616-45d2-a246-7dbb5429e1b9"},"source":["print(clf.predict(sample_test_matrix))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[-1  1  1  1 -1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FzIMZrFe5qD6"},"source":[" Recall from the lectures that we can also calculate the probability predictions from the scores using:\n"," <br>\n"," $$P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}) = \\dfrac{1}{1+\\exp{(-\\mathbf{w}^\\intercal h(\\mathbf{x}_i))}}$$"]},{"cell_type":"markdown","metadata":{"id":"h6CdJhsT5qD7"},"source":["We can use the predict_proba() function from scikit-learn."]},{"cell_type":"code","metadata":{"id":"6qFt_W8J5qD8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986153,"user_tz":-420,"elapsed":28109,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"6239661a-602f-4750-88ae-48bba3cf1dcb"},"source":["print(clf.predict_proba(sample_test_matrix))\n","# NOTE: P(yi=+1|xi,w) represented in the second column, the first column presents the P(yi=-1|xi,w)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[[0.99820671 0.00179329]\n"," [0.00219522 0.99780478]\n"," [0.01243515 0.98756485]\n"," [0.11873983 0.88126017]\n"," [0.9965412  0.0034588 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E6FDJdXl5qD_"},"source":["We can calculate the accuracy:\n","<br>\n","$$\\mbox{accuracy} = \\dfrac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}$$\n","<br>\n","We can use the score() function from scikit-learn."]},{"cell_type":"code","metadata":{"id":"csHAIKFk5qEA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986154,"user_tz":-420,"elapsed":28093,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"7011fe59-02b2-4f07-a95e-2c55e7b099d3"},"source":["# Accuracy of 6 samples above :DD\n","print(clf.score(sample_test_matrix, sample_test_data['sentiment']))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r5j7XFn35qEH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069986155,"user_tz":-420,"elapsed":28078,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"15218301-ef99-47f7-9198-5ae09f43579a"},"source":["# Accuracy for the hole test_data\n","print('Test accuracy: {}'.format(clf.score(test_matrix, test_data['sentiment'])))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Test accuracy: 0.9344247548799136\n"],"name":"stdout"}]}]}