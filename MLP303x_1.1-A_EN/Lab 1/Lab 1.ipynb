{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 1.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mv4_sVn42-To"},"source":["# Implementing logistic regression from scratch\n","\n","The goal of this notebook is to implement your own logistic regression classifier. You will:\n","\n"," * Extract features from Amazon product reviews.\n"," * Convert an pandas DataFrame into a NumPy array.\n"," * Implement the link function for logistic regression.\n"," * Write a function to compute the derivative of the log likelihood function with respect to a single coefficient.\n"," * Implement gradient ascent.\n"," * Given a set of coefficients, predict sentiments.\n"," * Compute classification accuracy for the logistic regression model.\n"," \n","Let's get started!"]},{"cell_type":"code","metadata":{"id":"KJ8SB1tt2-Tq"},"source":["# Import some libs\n","\n","import pandas\n","import numpy as np\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYI27oF12-Tv"},"source":["### Load review dataset\n","For this assignment, we will use a subset of the Amazon product review dataset. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews.\n","\n","Load the dataset into a data frame named **products**. One column of this dataset is **sentiment**, corresponding to the class label with +1 indicating a review with positive sentiment and -1 for negative sentiment.\n","\n","Let us quickly explore more of this dataset. The **name** column indicates the name of the product. Try listing the name of the first 10 products in the dataset.\n","\n","After that, try counting the number of positive and negative reviews."]},{"cell_type":"code","metadata":{"id":"fydZ2oYq2-Tw"},"source":["# Import amazon_baby.csv data to pandas dataframe\n","products_df = pandas.read_csv('/content/drive/MyDrive/FUNIX Progress/MLP303x_1.1-A_EN/data/amazon_baby_subset.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fv6wSZ0KCS_q","executionInfo":{"status":"ok","timestamp":1616773169944,"user_tz":-420,"elapsed":1658,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"fe0df264-2410-48a3-e74e-51b50d2cbdfe"},"source":["products_df[\"name\"][:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    Stop Pacifier Sucking without tears with Thumb...\n","1      Nature's Lullabies Second Year Sticker Calendar\n","2      Nature's Lullabies Second Year Sticker Calendar\n","3                          Lamaze Peekaboo, I Love You\n","4    SoftPlay Peek-A-Boo Where's Elmo A Children's ...\n","5                            Our Baby Girl Memory Book\n","6    Hunnt&reg; Falling Flowers and Birds Kids Nurs...\n","7    Blessed By Pope Benedict XVI Divine Mercy Full...\n","8    Cloth Diaper Pins Stainless Steel Traditional ...\n","9    Cloth Diaper Pins Stainless Steel Traditional ...\n","Name: name, dtype: object"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xg9tvrxZCaWN","executionInfo":{"status":"ok","timestamp":1616773169948,"user_tz":-420,"elapsed":1636,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"7a78ca77-d85f-4429-fb0b-e01c6b50eb69"},"source":["products_df.groupby('sentiment').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>review</th>\n","      <th>rating</th>\n","    </tr>\n","    <tr>\n","      <th>sentiment</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>-1</th>\n","      <td>26461</td>\n","      <td>26393</td>\n","      <td>26493</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>26521</td>\n","      <td>26438</td>\n","      <td>26579</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            name  review  rating\n","sentiment                       \n","-1         26461   26393   26493\n"," 1         26521   26438   26579"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"KGZHSNQa2-T3"},"source":["### Apply text cleaning on the review data\n","In this section, we will perform some simple feature cleaning using data frames. The last assignment used all words in building bag-of-words features, but here we limit ourselves to 193 words (for simplicity). We compiled a list of 193 most frequent words into the JSON file named **important_words.json**. Load the words into a list **important_words**."]},{"cell_type":"code","metadata":{"id":"ETAK3rkx2-T4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616773169952,"user_tz":-420,"elapsed":1618,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"7fc9cd1f-839d-42ca-82a9-2bf4a92c7848"},"source":["import json\n","\n","with open('/content/drive/MyDrive/FUNIX Progress/MLP303x_1.1-A_EN/data/important_words.json', 'r') as f:\n","    important_words = json.loads(f.read())\n","print(important_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['baby', 'one', 'great', 'love', 'use', 'would', 'like', 'easy', 'little', 'seat', 'old', 'well', 'get', 'also', 'really', 'son', 'time', 'bought', 'product', 'good', 'daughter', 'much', 'loves', 'stroller', 'put', 'months', 'car', 'still', 'back', 'used', 'recommend', 'first', 'even', 'perfect', 'nice', 'bag', 'two', 'using', 'got', 'fit', 'around', 'diaper', 'enough', 'month', 'price', 'go', 'could', 'soft', 'since', 'buy', 'room', 'works', 'made', 'child', 'keep', 'size', 'small', 'need', 'year', 'big', 'make', 'take', 'easily', 'think', 'crib', 'clean', 'way', 'quality', 'thing', 'better', 'without', 'set', 'new', 'every', 'cute', 'best', 'bottles', 'work', 'purchased', 'right', 'lot', 'side', 'happy', 'comfortable', 'toy', 'able', 'kids', 'bit', 'night', 'long', 'fits', 'see', 'us', 'another', 'play', 'day', 'money', 'monitor', 'tried', 'thought', 'never', 'item', 'hard', 'plastic', 'however', 'disappointed', 'reviews', 'something', 'going', 'pump', 'bottle', 'cup', 'waste', 'return', 'amazon', 'different', 'top', 'want', 'problem', 'know', 'water', 'try', 'received', 'sure', 'times', 'chair', 'find', 'hold', 'gate', 'open', 'bottom', 'away', 'actually', 'cheap', 'worked', 'getting', 'ordered', 'came', 'milk', 'bad', 'part', 'worth', 'found', 'cover', 'many', 'design', 'looking', 'weeks', 'say', 'wanted', 'look', 'place', 'purchase', 'looks', 'second', 'piece', 'box', 'pretty', 'trying', 'difficult', 'together', 'though', 'give', 'started', 'anything', 'last', 'company', 'come', 'returned', 'maybe', 'took', 'broke', 'makes', 'stay', 'instead', 'idea', 'head', 'said', 'less', 'went', 'working', 'high', 'unit', 'seems', 'picture', 'completely', 'wish', 'buying', 'babies', 'won', 'tub', 'almost', 'either']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ki-ypR4-2-T8"},"source":["Here we remove **remove_punctuation** and fill in N/A's with empty reivew"]},{"cell_type":"code","metadata":{"id":"CEnWi5Tu2-T8"},"source":["def remove_punctuation(text):\n","    import string\n","    return text.translate(text.maketrans('', '', string.punctuation))\n","\n","products_df = products_df.fillna({'review':''})  # fill in N/A's in the review column\n","products_df['review_clean'] = products_df['review'].apply(remove_punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gP4pklU2-UA"},"source":["Now we proceed with the second item. For each word in **important_words**, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in **important_words** which keeps a count of the number of times the respective word occurs in the review text."]},{"cell_type":"code","metadata":{"id":"JlIre7Oc2-UB","colab":{"base_uri":"https://localhost:8080/","height":731},"executionInfo":{"status":"ok","timestamp":1616773244239,"user_tz":-420,"elapsed":75867,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"04cc9f4c-45ff-4dd5-cab4-e5fa6fd9c4c2"},"source":["# 1-hot encoding\n","for word in important_words:\n","    products_df[word] = products_df['review_clean'].apply(lambda s : s.split().count(word))\n","\n","products_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>review</th>\n","      <th>rating</th>\n","      <th>sentiment</th>\n","      <th>review_clean</th>\n","      <th>baby</th>\n","      <th>one</th>\n","      <th>great</th>\n","      <th>love</th>\n","      <th>use</th>\n","      <th>would</th>\n","      <th>like</th>\n","      <th>easy</th>\n","      <th>little</th>\n","      <th>seat</th>\n","      <th>old</th>\n","      <th>well</th>\n","      <th>get</th>\n","      <th>also</th>\n","      <th>really</th>\n","      <th>son</th>\n","      <th>time</th>\n","      <th>bought</th>\n","      <th>product</th>\n","      <th>good</th>\n","      <th>daughter</th>\n","      <th>much</th>\n","      <th>loves</th>\n","      <th>stroller</th>\n","      <th>put</th>\n","      <th>months</th>\n","      <th>car</th>\n","      <th>still</th>\n","      <th>back</th>\n","      <th>used</th>\n","      <th>recommend</th>\n","      <th>first</th>\n","      <th>even</th>\n","      <th>perfect</th>\n","      <th>nice</th>\n","      <th>...</th>\n","      <th>looks</th>\n","      <th>second</th>\n","      <th>piece</th>\n","      <th>box</th>\n","      <th>pretty</th>\n","      <th>trying</th>\n","      <th>difficult</th>\n","      <th>together</th>\n","      <th>though</th>\n","      <th>give</th>\n","      <th>started</th>\n","      <th>anything</th>\n","      <th>last</th>\n","      <th>company</th>\n","      <th>come</th>\n","      <th>returned</th>\n","      <th>maybe</th>\n","      <th>took</th>\n","      <th>broke</th>\n","      <th>makes</th>\n","      <th>stay</th>\n","      <th>instead</th>\n","      <th>idea</th>\n","      <th>head</th>\n","      <th>said</th>\n","      <th>less</th>\n","      <th>went</th>\n","      <th>working</th>\n","      <th>high</th>\n","      <th>unit</th>\n","      <th>seems</th>\n","      <th>picture</th>\n","      <th>completely</th>\n","      <th>wish</th>\n","      <th>buying</th>\n","      <th>babies</th>\n","      <th>won</th>\n","      <th>tub</th>\n","      <th>almost</th>\n","      <th>either</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n","      <td>All of my kids have cried non-stop when I trie...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>All of my kids have cried nonstop when I tried...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n","      <td>We wanted to get something to keep track of ou...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>We wanted to get something to keep track of ou...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n","      <td>My daughter had her 1st baby over a year ago. ...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>My daughter had her 1st baby over a year ago S...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lamaze Peekaboo, I Love You</td>\n","      <td>One of baby's first and favorite books, and it...</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>One of babys first and favorite books and it i...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n","      <td>Very cute interactive book! My son loves this ...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>Very cute interactive book My son loves this b...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 198 columns</p>\n","</div>"],"text/plain":["                                                name  ... either\n","0  Stop Pacifier Sucking without tears with Thumb...  ...      0\n","1    Nature's Lullabies Second Year Sticker Calendar  ...      0\n","2    Nature's Lullabies Second Year Sticker Calendar  ...      0\n","3                        Lamaze Peekaboo, I Love You  ...      0\n","4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...  ...      0\n","\n","[5 rows x 198 columns]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"b5ICmtbv2-UE"},"source":["Now, write some code to compute the number of product reviews that contain the word perfect.\n","<br>\n","**Quiz Question:** How many reviews contain the word **\"perfect\"**?\n","<br>\n","**Your answer:** "]},{"cell_type":"code","metadata":{"id":"CBhWEPol2-UF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616773244241,"user_tz":-420,"elapsed":75842,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"a069c346-a8d6-41c2-af2f-c472c60e6fa9"},"source":["len(products_df[products_df['perfect'] != 0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2955"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"a15vXnIH2-UI"},"source":["### Convert data frame to multi-dimensional array\n","Write a function that extracts columns from a data frame and converts them into a multi-dimensional array. We plan to use them throughout the course, so make sure to get this function right.\n","<br>\n","The function should accept three parameters:\n","<br>\n","- **dataframe:** a data frame to be converted\n","- **features:** a list of string, containing the names of the columns that are used as features.\n","- **label:** a string, containing the name of the single column that is used as class labels.\n","<br>\n","<br>\n","The function should return two values:\n","<br>\n","- one 2D array for features\n","- one 1D array for class labels\n","<br>\n","<br>\n","The function should do the following:\n","<br>\n","- Prepend a new column constant to dataframe and fill it with 1's. This column takes account of the intercept term. Make sure that the constant column appears first in the data frame.\n","- Prepend a string 'constant' to the list features. Make sure the string 'constant' appears first in the list.\n","- Extract columns in dataframe whose names appear in the list features.\n","- Convert the extracted columns into a 2D array using a function in the data frame library. If you are using Pandas, you would use as_matrix() function.\n","- Extract the single column in dataframe whose name corresponds to the string label.\n","- Convert the column into a 1D array.\n","- Return the 2D array and the 1D array."]},{"cell_type":"code","metadata":{"id":"7XxOdfAH2-UI"},"source":["def get_numpy_data(dataframe, features, label):\n","    dataframe['constant'] = 1\n","\n","    features = ['constant'] + features\n","    \n","    feature_matrix = dataframe[features].values\n","    label_array = dataframe[label].values\n","\n","    return (feature_matrix, label_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yML8VvWh2-UL"},"source":["Using the function **get_numpy_data**, extract two arrays **feature_matrix** and **sentiment**. The 2D array **feature_matrix** would contain the content of the columns given by the list **important_words**. The 1D array sentiment would contain the content of the column **sentiment**."]},{"cell_type":"code","metadata":{"id":"Uq4Xz9C32-UM"},"source":["feature_matrix, sentiment = get_numpy_data(products_df, important_words, 'sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GqpKC7YM2-UP"},"source":["**Quiz Question:** How many features are there in the feature_matrix?\n","<br>\n","**Your answer:**"]},{"cell_type":"code","metadata":{"id":"et72vF_52-UQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616773244247,"user_tz":-420,"elapsed":75791,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"3c5d6c24-311b-44c6-a08e-434ec8320995"},"source":["feature_matrix.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(53072, 194)"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"5XBaYB592-US"},"source":["Recall from lecture that the link function is given by\n","<br>\n","$P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}) = \\dfrac{1}{1 + \\exp{(-\\mathbf{w}^\\intercal h(\\mathbf{x}_i))}}$\n","<br>\n","where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review $\\mathbf{x}_i$\n","<br>\n","Write a function named **predict_probability** that implements the link function.\n","<br>\n","- Take two parameters: **feature_matrix** and **coefficients**.\n","- First compute the dot product of **feature_matrix** and **coefficients**.\n","- Then compute the link function $P(y = +1 | \\mathbf{x}, \\mathbf{w})$\n","- Return the predictions given by the link function."]},{"cell_type":"code","metadata":{"id":"Ifbvlm-42-UT"},"source":["def predict_probability(feature_matrix, coefficients):\n","    # Take dot product of feature_matrix and coefficients\n","    scores = feature_matrix.dot(coefficients)\n","    \n","    # Compute P(y_i = +1 | x_i, w) using the link function\n","    predictions = 1 / (1 + np.exp(-scores))\n","    \n","    # return predictions\n","    return scores, predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d583ldkf2-UV"},"source":["**Aside**. How the link function works with matrix algebra\n","\n","Since the word counts are stored as columns in **feature_matrix**, each i-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n","$$[\\mbox{feature_matrix}] = \\left[\\begin{array}{c} h(\\mathbf{x}_1)^\\intercal \\\\ h(\\mathbf{x}_2)^\\intercal \\\\ \\vdots \\\\ h(\\mathbf{x}_N)^\\intercal\\end{array}\\right] = \\left[\\begin{array}{cccc}h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\ h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N) \\end{array}\\right]$$\n","By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^\\intercal h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$:\n","$$[\\mbox{score}] = [\\mbox{feature_matrix}]\\mathbf{w} = \\left[\\begin{array}{c} h(\\mathbf{x}_1)^\\intercal \\\\ h(\\mathbf{x}_2)^\\intercal \\\\ \\vdots \\\\ h(\\mathbf{x}_N)^\\intercal\\end{array}\\right] \\mathbf{w} = \\left[\\begin{array}{c} h(\\mathbf{x}_1)^\\intercal \\mathbf{w} \\\\ h(\\mathbf{x}_2)^\\intercal \\mathbf{w} \\\\ \\vdots \\\\ h(\\mathbf{x}_N)^\\intercal \\mathbf{w}\\end{array}\\right] = \\left[\\begin{array}{c} \\mathbf{w}^\\intercal h(\\mathbf{x}_1) \\\\ \\mathbf{w}^\\intercal h(\\mathbf{x}_2) \\\\ \\vdots \\\\ \\mathbf{w}^\\intercal h(\\mathbf{x}_N) \\end{array}\\right]$$"]},{"cell_type":"markdown","metadata":{"id":"crfssyN72-UW"},"source":["### Compute derivative of log likelihood with respect to a single coefficient\n","Recall from lecture:\n","$$\\displaystyle \\frac{\\partial \\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i) (\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}))$$\n","- **errors:** vector whose i-th value contains\n","$$\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$$\n","- **feature:** vector whose i-th value contains\n","$h_j(\\mathbf{x}_i)$\n","<br>\n","This corresponds to the j-th column of feature_matrix.\n","<br>\n","The function should do the following:\n","<br>\n","- Take two parameters **errors** and **feature**.\n","- Compute the dot product of **errors** and **feature**.\n","- Return the dot product. This is the derivative with respect to a single coefficient w_j."]},{"cell_type":"code","metadata":{"id":"tUrCxCXV2-UX"},"source":["def feature_derivative(errors, feature):     \n","    # Compute the dot product of errors and feature\n","    derivative = np.dot(np.transpose(errors), feature)\n","\n","    return derivative"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M0Y9-cKq2-Uc"},"source":["In the main lecture, our focus was on the likelihood. In the advanced optional video, however, we introduced a transformation of this likelihood---called the log-likelihood---that simplifies the derivation of the gradient and is more numerically stable. Due to its numerical stability, we will use the log-likelihood instead of the likelihood to assess the algorithm.\n","\n","The log-likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n","$$\\displaystyle \\ell \\ell (\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1) \\mathbf{w}^\\intercal h(\\mathbf{x}_i) - \\ln{\\big(1 + \\exp{(-\\mathbf{w}^\\intercal h(\\mathbf{x}_i) )} \\big)} \\Big)$$\n","<br>\n","Write a function **compute_log_likelihood** that implements the equation\n","<br>\n","The function has two parameters:\n","<br>\n","**indicator**: Has shape (N, 1). **indicator[i]** = True if **yi** = +1 otherwise **indicator[i]** = False\n","<br>\n","**scores**: The scores return by the **predict_probability** function. Refer to the above section for more details about the **scores** parameter."]},{"cell_type":"code","metadata":{"id":"chI-smK42-Ud"},"source":["def compute_log_likelihood(indicator, scores):\n","    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores)))\n","    return lp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YdoFCLoR2-Ui"},"source":["### Taking gradient steps\n","Now we are ready to implement our own logistic regression. All we have to do is to write a gradient ascent function that takes gradient steps towards the optimum.\n","<br>\n","Write a function **logistic_regression** to fit a logistic regression model using gradient ascent.\n","<br>\n","The function accepts the following parameters:\n","<br>\n","- **feature_matrix:** 2D array of features\n","- **sentiment:** 1D array of class labels\n","- **initial_coefficients:** 1D array containing initial values of coefficients\n","- **step_size:** a parameter controlling the size of the gradient steps\n","- **max_iter:** number of iterations to run gradient ascent\n","<br>\n","The function returns the last set of coefficients after performing gradient ascent.\n","<br>\n","The function carries out the following steps:\n","<br>\n","1. Initialize vector **coefficients** to **initial_coefficients**.\n","2. Predict the class probability $P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w})$ using your **predict_probability** function and save it to variable **predictions**.\n","3. Compute indicator value for $(y_i = +1)$ by comparing **sentiment** against +1. Save it to variable **indicator**.\n","4. Compute the errors as difference between **indicator** and **predictions**. Save the errors to variable **errors**.\n","5. For each j-th coefficient, compute the per-coefficient derivative by calling **feature_derivative** with the j-th column of **feature_matrix**. Then increment the j-th coefficient by (step_size*derivative).\n","6. Once in a while, insert code to print out the log likelihood.\n","7. Repeat steps 2-6 for **max_iter** times."]},{"cell_type":"code","metadata":{"id":"9XuXtt4m2-Uj"},"source":["from math import sqrt\n","def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n","    coefficients = initial_coefficients # make sure it's a numpy array\n","\n","    for itr in range(max_iter):\n","        # Predict P(y_i = +1|x_1,w) using your predict_probability() function\n","        scores, predictions = predict_probability(feature_matrix, coefficients)\n","\n","        # Compute indicator value for (y_i = +1)\n","        indicator = (sentiment == +1) # .reshape(predictions.shape[0], 1)\n","\n","        # Compute the errors as indicator - predictions\n","        errors = indicator - predictions\n","\n","        for j in range(coefficients.shape[0]): # loop over each coefficient\n","            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n","            # compute the derivative for coefficients[j]. Save it in a variable called derivative\n","            derivative = feature_derivative(errors, feature_matrix[:,j])\n","\n","            # add the step size times the derivative to the current coefficient\n","            coefficients[j] += step_size * derivative\n","        \n","        if itr % 100 == 0:\n","            lp = compute_log_likelihood(indicator, scores)\n","            print ('iteration: {}, log likelihood: {}'.format(itr, lp))\n","\n","    return coefficients"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6s2wLhxIKEH4"},"source":["# Advance\n","def self_make_logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n","    coefficients = initial_coefficients\n","\n","    for itr in range(max_iter):\n","        scores, predictions = predict_probability(feature_matrix, coefficients)\n","\n","        indicator = (sentiment == +1)\n","\n","        errors = indicator - predictions\n","        \n","        # Modify here\n","        coefficients += step_size * feature_derivative(errors, feature_matrix)\n","        \n","        if itr % 100 == 0:\n","            lp = compute_log_likelihood(indicator, scores)\n","            print ('iteration: {}, log likelihood: {}'.format(itr, lp))\n","\n","    return coefficients"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlArg4Va2-Ul"},"source":["Now, let us run the logistic regression solver with the parameters below:\n","<br>\n","- **feature_matrix** = feature_matrix extracted\n","- **sentiment** = sentiment extracted\n","- **initial_coefficients** = a 194-dimensional vector filled with zeros\n","- **step_size** = 1e-7\n","- **max_ite**r = 301\n","<br>\n","Save the returned **coefficients** to variable **coefficients**."]},{"cell_type":"code","metadata":{"id":"FMZBpsXE2-Uq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616773262809,"user_tz":-420,"elapsed":94230,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"06c93c51-094c-4cb1-a938-c3851e78de85"},"source":["initial_coefficients = np.zeros((feature_matrix.shape[1],))\n","step_size = 1e-7\n","max_iter = 301\n","coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iteration: 0, log likelihood: -36786.70716667741\n","iteration: 100, log likelihood: -36235.70757728472\n","iteration: 200, log likelihood: -35733.69577367936\n","iteration: 300, log likelihood: -35272.93069887046\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U22vJrfuL4qB","executionInfo":{"status":"ok","timestamp":1616773278240,"user_tz":-420,"elapsed":109641,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"6355dee0-7506-4ac0-ca72-0221cb5fe975"},"source":["self_make_coefficients = self_make_logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iteration: 0, log likelihood: -35268.51212682766\n","iteration: 100, log likelihood: -34844.003998221226\n","iteration: 200, log likelihood: -34451.20771660703\n","iteration: 300, log likelihood: -34086.653432478925\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kW9i6U3NL91Q","executionInfo":{"status":"ok","timestamp":1616773278242,"user_tz":-420,"elapsed":109625,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"4ec936a5-b115-4c95-a058-d40b3b938579"},"source":["coefficients - self_make_coefficients"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"skt6Jk402-Us"},"source":["**Quiz question:** As each iteration of gradient ascent passes, does the log likelihood increase or decrease?\n","<br>\n","**Your answer: Increase**"]},{"cell_type":"markdown","metadata":{"id":"sITowFCT2-Ut"},"source":["### Predicting sentiments\n","Recall from lecture that class predictions for a data point x can be computed from the coefficients w using the following formula:\n","<br>\n","$$\\hat{y}_i = \\begin{cases} +1 & \\text{if }\\mathbf{x}_i^\\intercal \\mathbf{w} > 0 \\\\ -1 & \\text{if }\\mathbf{x}_i^\\intercal \\mathbf{w} \\leq 0\\end{cases}$$\n","Now, we write some code to compute class predictions. We do this in two steps:\n","- First compute the scores using feature_matrix and coefficients using a dot product.\n","- Then apply threshold 0 on the scores to compute the class predictions. Refer to the formula above.\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"Hb2ZtZMx2-Ut"},"source":["**Quiz question:** How many reviews were predicted to have positive sentiment?\n","<br>\n","**Your answer:**"]},{"cell_type":"code","metadata":{"id":"HKCpRJeu2-Uu"},"source":["# scores = feature_matrix.dot(coefficients)\n","scores, predictions = predict_probability(feature_matrix, coefficients)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAKJFfTS2-Uw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616773278244,"user_tz":-420,"elapsed":109597,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"eafc133d-1515-4404-b336-238f751816fe"},"source":["len(predictions[predictions > 0.5])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26452"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"HEbATRU52-Uz"},"source":["### Measuring accuracy\n","We will now measure the classification accuracy of the model. Recall from the lecture that the classification accuracy can be computed as follows:\n","$$\\mbox{accuracy} = \\dfrac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}$$\n","**Quiz question:** hat is the accuracy of the model on predictions made above? (round to 2 digits of accuracy)\n","<br>\n","**Your answer: 0.75**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sgKZzI5agQA","executionInfo":{"status":"ok","timestamp":1616773278247,"user_tz":-420,"elapsed":109582,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"a40d2490-5d1a-4826-a98e-eaf4ea40a185"},"source":["diff = (scores > 0) == (sentiment > 0)\n","print ('Accuracy: {}'.format(np.sum(diff) / len(diff)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.7548047934880916\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"puG0NtjQ2-U1"},"source":["Recall that in the earlier assignment, we were able to compute the \"**most positive words**\". These are words that correspond most strongly with positive reviews. In order to do this, we will first do the following:\n","- Treat each coefficient as a tuple, i.e. (**word, coefficient_value**). The intercept has no corresponding word, so throw it out.\n","- Sort all the (**word, coefficient_value**) tuples by **coefficient_value** in descending order. Save the sorted list of tuples to **word_coefficient_tuples**."]},{"cell_type":"markdown","metadata":{"id":"J-k_pHG22-U2"},"source":["Now, **word_coefficient_tuples** contains a sorted list of (**word, coefficient_value**) tuples. The first 10 elements in this list correspond to the words that are most positive.\n","<br>\n","**Quiz question:** What is the top 10 positive words?\n","<br>\n","**Your answer:**\n","<br>\n","**Quiz question:** What is the top 10 negative words?\n","<br>\n","**Your answer:**"]},{"cell_type":"code","metadata":{"id":"VP-YpK7f2-U2"},"source":["coefficients = list(coefficients[1:]) # exclude intercept\n","word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients)]\n","word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqi85VIJ2-U5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616773278249,"user_tz":-420,"elapsed":109546,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"87db9daf-4745-403d-aed3-5b1f8410ee75"},"source":["word_coefficient_tuples[:11]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('great', 0.12617483799732268),\n"," ('love', 0.125265596682835),\n"," ('easy', 0.1232886335285767),\n"," ('loves', 0.08677892963900478),\n"," ('little', 0.08648965018920975),\n"," ('well', 0.05821798832990093),\n"," ('perfect', 0.05785064299368017),\n"," ('old', 0.03806010368745643),\n"," ('nice', 0.03589639699960394),\n"," ('daughter', 0.034298130985858315),\n"," ('soft', 0.03395785092036801)]"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vb58ryjaa6RI","executionInfo":{"status":"ok","timestamp":1616773278251,"user_tz":-420,"elapsed":109521,"user":{"displayName":"Khanh Vương","photoUrl":"","userId":"11298773691448526127"}},"outputId":"78a8442b-7500-4dd6-c087-63ffe422bebd"},"source":["word_coefficient_tuples[-10:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('waste', -0.04624412301305021),\n"," ('back', -0.050620242563483915),\n"," ('get', -0.050983258735577776),\n"," ('return', -0.05103450496244153),\n"," ('even', -0.055659458248917736),\n"," ('disappointed', -0.05632740024143928),\n"," ('work', -0.0617400094008335),\n"," ('money', -0.07426071913860344),\n"," ('product', -0.07453419871769011),\n"," ('would', -0.09446749481057254)]"]},"metadata":{"tags":[]},"execution_count":67}]}]}