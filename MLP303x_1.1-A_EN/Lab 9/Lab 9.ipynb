{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 9.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14w_JQrFP9jZn6kiqNtABLqUwyLh-xnp1","authorship_tag":"ABX9TyOuFMYz167C9IvxsQou9INm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LFv6wecrOus4"},"source":["# Naive Bayes and Random forest with scikit-learn\n","\n","The goal of this notebook is to a random forest model using scikit-learn library to identifying safe loans. You will do the following:\n","\n"," * Load safe loans datasetse.\n"," * Implement random forest model using scikit-learn.\n"," * Tuning some parameters."]},{"cell_type":"code","metadata":{"id":"vAiTgrL3OutH"},"source":["# Import some libs\n","\n","import pandas\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_IU_xnwOutJ"},"source":["## Load product reviews dataset\n","Like previous module, we load, preprocess data, convert and split them into train and test datasets. We dont't focus on that in this notebook, so you can just run the following cells. You can check out the load data code inside the folder **utils**."]},{"cell_type":"code","metadata":{"id":"GVQjm1BUPLep"},"source":["def get_numpy_data(dataframe, features, label):\n","    dataframe.loc[:, 'intercept'] = 1\n","    features = ['intercept'] + features\n","    feature_matrix = dataframe.loc[:, features].values\n","    label_array = dataframe.loc[:, label].values\n","    return (feature_matrix, label_array)\n","\n","def remove_punctuation(text):\n","    import string\n","    return text.translate(string.punctuation)\n","\n","def get_product_reviews_data():\n","    products_df = pandas.read_csv('datasets/amazon_baby_subset.csv')\n","\n","    with open('datasets/important_words.json', 'r') as f:\n","        important_words = json.loads(f.read())\n","\n","    products_df = products_df.fillna({'review':''})  # fill in N/A's in the review column\n","    products_df.loc[:, 'review_clean'] = products_df['review'].apply(remove_punctuation)\n","\n","    for word in important_words:\n","        products_df.loc[:, word] = products_df['review_clean'].apply(lambda s : s.split().count(word))\n","\n","    sentiment_train_data = products_df.sample(frac=0.8, random_state=100)\n","    sentiment_validation_data = products_df.drop(sentiment_train_data.index)\n","\n","    sentiment_X_train, sentiment_y_train = get_numpy_data(sentiment_train_data, important_words, 'sentiment')\n","    sentiment_X_valid, sentiment_y_valid = get_numpy_data(sentiment_validation_data, important_words, 'sentiment')\n","\n","    print ('*****Sentiment data shape*****')\n","    print ('sentiment_X_train.shape: ', sentiment_X_train.shape)\n","    print ('sentiment_y_train.shape: ', sentiment_y_train.shape)\n","    print ('sentiment_X_valid.shape: ', sentiment_X_valid.shape)\n","    print ('sentiment_y_valid.shape: ', sentiment_y_valid.shape)\n","\n","    return (sentiment_X_train, sentiment_y_train), (sentiment_X_valid, sentiment_y_valid)\n","def get_loans_data():\n","    loans_df = pandas.read_csv('/content/drive/MyDrive/FUNIX Progress/MLP303x_1.1-A_EN/data/lending-club-data.csv', low_memory=False)\n","    # safe_loans =  1 => safe\n","    # safe_loans = -1 => risky\n","    loans_df.loc[:, 'safe_loans'] = loans_df['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n","    loans_df.drop(columns=['bad_loans'])\n","    target = 'safe_loans'\n","    features = ['grade',                     # grade of the loan (categorical)\n","                'sub_grade_num',             # sub-grade of the loan as a number from 0 to 1\n","                'short_emp',                 # one year or less of employment\n","                'emp_length_num',            # number of years of employment\n","                'home_ownership',            # home_ownership status: own, mortgage or rent\n","                'dti',                       # debt to income ratio\n","                'purpose',                   # the purpose of the loan\n","                'payment_inc_ratio',         # ratio of the monthly payment to income\n","                'delinq_2yrs',               # number of delinquincies\n","                'delinq_2yrs_zero',          # no delinquincies in last 2 years\n","                'inq_last_6mths',            # number of creditor inquiries in last 6 months\n","                'last_delinq_none',          # has borrower had a delinquincy\n","                'last_major_derog_none',     # has borrower had 90 day or worse rating\n","                'open_acc',                  # number of open credit accounts\n","                'pub_rec',                   # number of derogatory public records\n","                'pub_rec_zero',              # no derogatory public records\n","                'revol_util',                # percent of available credit being used\n","                'total_rec_late_fee',        # total late fees received to day\n","                'int_rate',                  # interest rate of the loan\n","                'total_rec_int',             # interest received to date\n","                'annual_inc',                # annual income of borrower\n","                'funded_amnt',               # amount committed to the loan\n","                'funded_amnt_inv',           # amount committed by investors for the loan\n","                'installment',               # monthly payment owed by the borrower\n","            ]\n","     # drop na\n","    loans_df = loans_df[features + [target]].dropna()\n","\n","    safe_loans_raw = loans_df[loans_df[target] == +1]\n","    risky_loans_raw = loans_df[loans_df[target] == -1]\n","\n","    # Since there are fewer risky loans than safe loans, find the ratio of the sizes\n","    # and use that percentage to undersample the safe loans.\n","    percentage = risky_loans_raw.shape[0]/safe_loans_raw.shape[0]\n","\n","    risky_loans = risky_loans_raw\n","    safe_loans = safe_loans_raw.sample(frac=percentage, random_state=1)\n","\n","    # Append the risky_loans with the downsampled version of safe_loans\n","    loans_data = risky_loans.append(safe_loans)\n","\n","    categorical_variables = list(loans_data.select_dtypes(include=['object']).columns)\n","\n","    one_hot_data = pandas.get_dummies(loans_data[categorical_variables], prefix=categorical_variables)\n","    # need to add inplace in oreder to drop columns.\n","    loans_data.drop(columns=categorical_variables, axis=1, inplace=True)\n","    loans_data = pandas.concat([loans_data, one_hot_data], axis=1)\n","\n","    loans_train_data = loans_data.sample(frac=0.8, random_state=100)\n","    loans_validation_data = loans_data.drop(loans_train_data.index)\n","\n","    loans_feature_colums = list(loans_train_data.columns)\n","    loans_feature_colums.remove(target)\n","    loans_X_train, loans_y_train = get_numpy_data(loans_train_data, loans_feature_colums, target)\n","    loans_X_valid, loans_y_valid = get_numpy_data(loans_validation_data, loans_feature_colums, target)\n","\n","    print ('*****Loans data shape*****')\n","    print ('loans_X_train.shape: ', loans_X_train.shape)\n","    print ('loans_y_train.shape: ', loans_y_train.shape)\n","    print ('loans_X_valid.shape: ', loans_X_valid.shape)\n","    print ('loans_y_valid.shape: ', loans_y_valid.shape)\n","\n","    return (loans_X_train, loans_y_train), (loans_X_valid, loans_y_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYeEOafcOutK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620448288138,"user_tz":-420,"elapsed":2997,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"fa6ecd72-5167-4f41-e6c1-4e9079d63327"},"source":["train_set, val_set = get_loans_data()\n","\n","loans_X_train, loans_y_train = train_set\n","loans_X_valid, loans_y_valid = val_set"],"execution_count":null,"outputs":[{"output_type":"stream","text":["*****Loans data shape*****\n","loans_X_train.shape:  (37035, 45)\n","loans_y_train.shape:  (37035,)\n","loans_X_valid.shape:  (9259, 45)\n","loans_y_valid.shape:  (9259,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TDzXb_-gOutL"},"source":["# Build Random forest classifiers using scikit learn\n","Now, let's use the built-in Naive Bayes learner [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n","## Choosing hyper parameters\n","There are two important hyper parameters when building a random forest classifier which are the number of decision trees and the number of random features at each split; they are corresponding to **n_estimators** and **max_features** in RandomForestClassifier class in scikit-learn. We will use **max_features = 'auto'** which is equal to **sqrt(n_features)**."]},{"cell_type":"code","metadata":{"id":"BtIlDGR9OutM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620448345029,"user_tz":-420,"elapsed":53567,"user":{"displayName":"Khanh Vuong Tuan","photoUrl":"","userId":"10783698511148593600"}},"outputId":"108eeb9c-7e98-42ac-a72b-596bbc55d46b"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rf_loans = RandomForestClassifier(n_estimators=500)\n","\n","rf_loans.fit(loans_X_train, loans_y_train)\n","\n","print (\"***Loans result***\")\n","print(\"Train accuracy: {}\".format(rf_loans.score(loans_X_train, loans_y_train)))\n","print(\"Validation accuracy: {}\".format(rf_loans.score(loans_X_valid, loans_y_valid)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***Loans result***\n","Train accuracy: 1.0\n","Validation accuracy: 0.6662706555783562\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LtxRwbqlOutM"},"source":["As you can see the random forest classifiers take quite a long time to train.\n","<br>\n","**Quiz**: What is the validation accuracy?\n","<br>\n","**Your answer**: 0.67"]}]}